# https://github.com/apache/airflow/blob/main/chart/values.yaml

useStandardNaming: true

createUserJob:
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync
migrateDatabaseJob:
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync

# config:
#   core:
#     simple_auth_manager_all_admins: "True"

extraEnv: |
  - name: "AIRFLOW__API__BASE_URL"
    value: "https://data-airflow.{{ .StateValues.ingress_hostname }}"

  - name: "CLIENT_ID"
    value: "airflow"
  - name: "CLIENT_SECRET"
    value: "{{ .StateValues.security_github_client_secret }}"
  - name: "OIDC_ISSUER"
    value: "https://security-dex.{{ .StateValues.ingress_hostname }}/dex"

  - name: "TRINO_HOST"
    value: "{{ $.Values.release_name }}-data-trino-trino:8080"
  - name: "TRINO_PORT"
    value: "8080"
  - name: "TRINO_USER"
    value: "{{ $.StateValues.trino_user}}"
  - name: "S3_AWS_ENDPOINT"
    value: "http://{{ .Values.release_name }}-data-minio:9000"
  - name: "S3_AWS_ACCESS_KEY"
    value: "{{ .StateValues.minio_user }}"
  - name: "S3_AWS_SECRET_KEY"
    valueFrom:
      secretKeyRef:
        name: minio-admin-credentials
        key: accessKey

postgresql:
  enabled: false
redis:
  enabled: false

data:
  metadataSecretName: postgresql-admin-credentials
  brokerUrlSecretName: valkey-admin-credentials

apiServer:
  args: 
    - "bash"
    - "-c"
    - |
      pip install uvicorn==0.29.0 && \ # Pinning uvicorn to avoid bug in 0.30.0 and up
      exec airflow api-server
  apiServerConfig: |
    {{- readFile "../files/scripts/airflow/webserver_config.py" | nindent 4 }}
  resources:
    # Homelab-friendly sizing: keep requests modest so the scheduler can pack
    # other services onto small clusters, while allowing a reasonable burst
    # via limits for short-lived heavier work.
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

workers:
  replicas: 2

  resources:
    # Workers can be a bit heavier, but for homelabs we keep per-worker
    # requests small and limits moderate so multiple workers can run.
    limits:
      cpu: 1
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 512Mi

  persistence:
    enabled: true
    size: 2Gi
    storageClassName: longhorn-no-replica

  logGroomerSidecar: &logGroomerSidecar
    enabled: true
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
      requests:
        cpu: 100m
        memory: 128Mi

dags:
  persistence:
    enabled: true
    size: 1Gi
    storageClassName: nfs
  gitSync:
    enabled: true

    repo: https://github.com/datahub-local/datahub-local-workflows.git
    branch: main
    rev: HEAD
    subPath: "worfklows/dags"

    sshKeySecret: github-airflow-auth
    period: 60s

dagProcessor:
  resources:
    # Dag processor should be responsive but not overly sized on homelab
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 512Mi

statsd:
  resources:
    # Statsd is lightweight; give low request/limit to reduce overall footprint
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 100m
      memory: 128Mi

scheduler:
  resources:
    # Scheduler handles task scheduling; modest sizing is sufficient for homelab
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 512Mi

  logGroomerSidecar: *logGroomerSidecar

triggerer:
  resources:
    # Triggerer handles task triggers; modest sizing is sufficient for homelab
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 250m
      memory: 512Mi

  logGroomerSidecar: *logGroomerSidecar

  persistence:
    enabled: true
    size: 0.5Gi
    storageClassName: longhorn-no-replica

ingress:
  apiServer:
    enabled: true

    annotations:
      gethomepage.dev/enabled: "true"
      gethomepage.dev/name: "Airflow"
      gethomepage.dev/pod-selector: "component=api-server,tier=airflow"
      gethomepage.dev/description: "A platform to programmatically author, schedule, and monitor workflows"
      gethomepage.dev/group: "Data"
      gethomepage.dev/icon: "si-apacheairflow"

    ingressClassName: "nginx"
    path: "/"
    pathType: Prefix
    hosts:
      - name: "data-airflow.{{ .StateValues.ingress_hostname }}"
