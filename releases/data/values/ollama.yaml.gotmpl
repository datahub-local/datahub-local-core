# https://github.com/otwld/ollama-helm/blob/main/values.yaml
ollama:
  gpu:
    enabled: true
    type: "nvidia"
    number: 1
  models:
    pull:
      - gemma3:4b-it-qat
      - phi4-mini:3.8b
      - qwen2.5-coder:1.5b-base
      - nomic-embed-text

updateStrategy:
  type: Recreate

runtimeClassName: nvidia

hostNetwork: true

persistentVolume:
  enabled: true
  storageClass: local-path

tolerations:
  - key: nvidia.com/gpu
    operator: Exists
    effect: NoSchedule

ingress:
  enabled: false

extraObjects:
  - apiVersion: traefik.io/v1alpha1
    kind: IngressRoute
    metadata:
      annotations:
        gethomepage.dev/enabled: "true"
        gethomepage.dev/name: "Ollama"
        gethomepage.dev/pod-selector: "app.kubernetes.io/name=ollama"
        gethomepage.dev/description: "Get up and running with Llama 3, Mistral, Gemma 2, and other large language models."
        gethomepage.dev/group: "Data"
        gethomepage.dev/icon: "sh-ollama"
      name: '{{ $.Values.release_name }}-ollama'
      namespace: data
    spec:
      entryPoints:
        - websecure
      routes:
        - match: Host(`data-ollama.{{ .StateValues.gateway_hostname }}`)
          kind: Rule
          services:
            - name: '{{ $.Values.release_name }}-ollama'
              port: http
          middlewares:
            - name: oauth2-proxy-auth
              namespace: security
        - match: Host(`data-ollama.{{ .StateValues.gateway_hostname }}`) && PathPrefix(`/oauth2/`)
          kind: Rule
          services:
            - name: oauth2-proxy
              namespace: security
              port: http
